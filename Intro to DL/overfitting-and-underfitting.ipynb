{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1480608,"sourceType":"datasetVersion","datasetId":829369}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Setup plotting\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')\n\n# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.deep_learning_intro.ex4 import *","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T03:17:58.209961Z","iopub.execute_input":"2025-10-16T03:17:58.210276Z","iopub.status.idle":"2025-10-16T03:17:59.854829Z","shell.execute_reply.started":"2025-10-16T03:17:58.210246Z","shell.execute_reply":"2025-10-16T03:17:59.853373Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/1500229236.py:3: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import GroupShuffleSplit\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Load and preprocess the data\nspotify = pd.read_csv('../input/dl-course-data/spotify.csv')\nX = spotify.copy().dropna()\ny = X.pop('track_popularity')\nartists = X['track_artist']\n\nfeatures_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n                'speechiness', 'acousticness', 'instrumentalness',\n                'liveness', 'valence', 'tempo', 'duration_ms']\nfeatures_cat = ['playlist_genre']\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), features_num),\n    (OneHotEncoder(), features_cat),\n)\n\n# Grouped split function\ndef group_split(X, y, group, train_size=0.75):\n    splitter = GroupShuffleSplit(train_size=train_size)\n    train, test = next(splitter.split(X, y, groups=group))\n    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n\nX_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\ny_train = y_train / 100\ny_valid = y_valid / 100\n\ninput_shape = [X_train.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))\n\n# Convert to PyTorch Tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\nX_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\ny_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).view(-1, 1)\n\n# Create TensorDatasets and DataLoaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nvalid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n\n# Define the PyTorch model\nclass SpotifyPopularityModel(nn.Module):\n    def __init__(self, input_size):\n        super(SpotifyPopularityModel, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_size, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1)\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\n# Instantiate the model\ninput_size = input_shape[0]\nmodel = SpotifyPopularityModel(input_size)\n\n# Define loss function and optimizer\ncriterion = nn.L1Loss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop with validation and early stopping\nearly_stopping = 10\nbest_val_loss = float('inf')\nepochs_no_improve = 0\nnum_epochs = 100\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n\n    # Validation step\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in valid_loader:\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(valid_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n\n    # Early stopping logic\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), 'best_model.pt')\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve == early_stopping:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n# Load the best model and evaluate\nmodel.load_state_dict(torch.load('best_model.pt'))\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T03:18:22.757068Z","iopub.execute_input":"2025-10-16T03:18:22.757402Z","iopub.status.idle":"2025-10-16T03:19:39.784673Z","shell.execute_reply.started":"2025-10-16T03:18:22.757379Z","shell.execute_reply":"2025-10-16T03:19:39.783871Z"}},"outputs":[{"name":"stdout","text":"Input shape: [18]\nEpoch [1/100], Train Loss: 0.2019, Validation Loss: 0.1991\nEpoch [2/100], Train Loss: 0.1950, Validation Loss: 0.1955\nEpoch [3/100], Train Loss: 0.1932, Validation Loss: 0.1963\nEpoch [4/100], Train Loss: 0.1913, Validation Loss: 0.1953\nEpoch [5/100], Train Loss: 0.1903, Validation Loss: 0.1948\nEpoch [6/100], Train Loss: 0.1890, Validation Loss: 0.1973\nEpoch [7/100], Train Loss: 0.1877, Validation Loss: 0.1962\nEpoch [8/100], Train Loss: 0.1866, Validation Loss: 0.1948\nEpoch [9/100], Train Loss: 0.1852, Validation Loss: 0.1967\nEpoch [10/100], Train Loss: 0.1844, Validation Loss: 0.1959\nEpoch [11/100], Train Loss: 0.1830, Validation Loss: 0.1970\nEpoch [12/100], Train Loss: 0.1811, Validation Loss: 0.1989\nEpoch [13/100], Train Loss: 0.1795, Validation Loss: 0.1970\nEpoch [14/100], Train Loss: 0.1778, Validation Loss: 0.1982\nEpoch [15/100], Train Loss: 0.1761, Validation Loss: 0.1972\nEpoch [16/100], Train Loss: 0.1742, Validation Loss: 0.1991\nEpoch [17/100], Train Loss: 0.1728, Validation Loss: 0.2025\nEpoch [18/100], Train Loss: 0.1706, Validation Loss: 0.2014\nEarly stopping at epoch 18\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"SpotifyPopularityModel(\n  (network): Sequential(\n    (0): Linear(in_features=18, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=512, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=512, out_features=1, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}