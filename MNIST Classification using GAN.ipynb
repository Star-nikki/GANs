{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:31:03.214728Z","iopub.execute_input":"2025-09-04T05:31:03.215291Z","iopub.status.idle":"2025-09-04T05:31:09.856168Z","shell.execute_reply.started":"2025-09-04T05:31:03.215264Z","shell.execute_reply":"2025-09-04T05:31:09.855022Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from torchvision import datasets, transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:31:09.857321Z","iopub.execute_input":"2025-09-04T05:31:09.857708Z","iopub.status.idle":"2025-09-04T05:31:16.325164Z","shell.execute_reply.started":"2025-09-04T05:31:09.857684Z","shell.execute_reply":"2025-09-04T05:31:16.323833Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-09-04T05:31:16.327980Z","iopub.execute_input":"2025-09-04T05:31:16.328454Z","iopub.status.idle":"2025-09-04T05:31:16.336260Z","shell.execute_reply.started":"2025-09-04T05:31:16.328427Z","shell.execute_reply":"2025-09-04T05:31:16.334279Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torchmetrics\nfrom accelerate import Accelerator\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:31:16.340257Z","iopub.execute_input":"2025-09-04T05:31:16.341186Z","iopub.status.idle":"2025-09-04T05:31:24.341522Z","shell.execute_reply.started":"2025-09-04T05:31:16.341147Z","shell.execute_reply":"2025-09-04T05:31:24.339761Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_mnist_classification_data(batch_size=64):\n    image_to_tensor_transform = transforms.ToTensor()\n    train_dataset = datasets.MNIST('data', train=True, download=True, transform=image_to_tensor_transform)\n    test_dataset = datasets.MNIST('data', train=False, transform=image_to_tensor_transform)\n    \n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_dataloader, test_dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:35:34.625643Z","iopub.execute_input":"2025-09-04T05:35:34.626176Z","iopub.status.idle":"2025-09-04T05:35:34.633133Z","shell.execute_reply.started":"2025-09-04T05:35:34.626138Z","shell.execute_reply":"2025-09-04T05:35:34.631923Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Classification Network","metadata":{}},{"cell_type":"code","source":"class DigitClassificationNetwork(nn.Module):\n    def __init__(self, input_size=784, hidden_size_1=256, hidden_size_2=128, num_classes=10):\n        super().__init__()\n        \n        self.flatten_image_layer = nn.Flatten()  # Convert 28x28 to 784\n        self.first_hidden_layer = nn.Linear(input_size, hidden_size_1)\n        self.first_activation = nn.ReLU()\n        self.first_dropout = nn.Dropout(0.2)  # Prevent overfitting\n        \n        self.second_hidden_layer = nn.Linear(hidden_size_1, hidden_size_2)\n        self.second_activation = nn.ReLU()\n        self.second_dropout = nn.Dropout(0.2)\n        \n        self.output_classification_layer = nn.Linear(hidden_size_2, num_classes)\n        print(f\"Created classification network: {input_size} -> {hidden_size_1} -> {hidden_size_2} -> {num_classes}\")\n    \n    def forward(self, input_images):\n        # Forward Pass\n        flattened_images = self.flatten_image_layer(input_images)      # Flatten the 28x28 images to 784 dimensional vectors\n        \n        first_layer_output = self.first_hidden_layer(flattened_images)\n        first_activated_output = self.first_activation(first_layer_output)\n        first_dropout_output = self.first_dropout(first_activated_output)\n        \n        second_layer_output = self.second_hidden_layer(first_dropout_output)\n        second_activated_output = self.second_activation(second_layer_output)\n        second_dropout_output = self.second_dropout(second_activated_output)\n        \n        classification_logits = self.output_classification_layer(second_dropout_output)\n        \n        return classification_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:35:47.161902Z","iopub.execute_input":"2025-09-04T05:35:47.162294Z","iopub.status.idle":"2025-09-04T05:35:47.170254Z","shell.execute_reply.started":"2025-09-04T05:35:47.162269Z","shell.execute_reply":"2025-09-04T05:35:47.169369Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Validating if Neural Network is capable of learning","metadata":{}},{"cell_type":"code","source":"def overfit_single_batch_classification(model, train_dataloader, optimizer, accelerator, iterations=200):\n    accelerator.print(\"=== OVERFITTING TEST: Training classifier on single batch ===\")\n    \n    # Get one single batch and keep using it\n    single_batch_images, single_batch_labels = next(iter(train_dataloader))\n    accelerator.print(f\"Using single batch with {single_batch_images.size(0)} images\")\n    \n    # Print the true labels in the batch for reference\n    unique_labels = torch.unique(single_batch_labels).cpu().numpy()\n    accelerator.print(f\"Digits in this batch: {unique_labels}\")\n    \n    loss_function = nn.CrossEntropyLoss()\n    model.train()  # Set model to training mode\n    \n    # Train on the same batch repeatedly\n    for iteration in range(iterations):\n        # Forward pass: get predictions on same batch\n        predicted_logits = model(single_batch_images)\n        training_loss = loss_function(predicted_logits, single_batch_labels)\n        \n        # Backward pass: update weights\n        optimizer.zero_grad()\n        accelerator.backward(training_loss)\n        optimizer.step()\n        \n        # Calculate accuracy on this batch\n        predicted_classes = torch.argmax(predicted_logits, dim=1)\n        correct_predictions = (predicted_classes == single_batch_labels).sum().item()\n        batch_accuracy = (correct_predictions / single_batch_labels.size(0)) * 100\n        \n        # Print progress every 25 iterations\n        if (iteration + 1) % 25 == 0:\n            accelerator.print(f'Iteration {iteration+1:3d}: Loss = {training_loss:.4f}, Accuracy = {batch_accuracy:.1f}%')\n    \n    # Final evaluation on the overfitted batch\n    model.eval()\n    with torch.no_grad():\n        final_predictions = model(single_batch_images)\n        final_predicted_classes = torch.argmax(final_predictions, dim=1)\n        final_accuracy = (final_predicted_classes == single_batch_labels).sum().item() / single_batch_labels.size(0) * 100\n    \n    accelerator.print(f\"Final overfitting accuracy: {final_accuracy:.1f}%\")\n    accelerator.print(\"If accuracy reaches ~100%, the network can learn. If not, check architecture/learning rate.\")\n    \n    model.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:36:03.570051Z","iopub.execute_input":"2025-09-04T05:36:03.570378Z","iopub.status.idle":"2025-09-04T05:36:03.582553Z","shell.execute_reply.started":"2025-09-04T05:36:03.570355Z","shell.execute_reply":"2025-09-04T05:36:03.580882Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_classification_model(model, train_dataloader, optimizer, accelerator, epochs=10):\n    loss_function = nn.CrossEntropyLoss()\n    \n    model.train()  # Set model to training mode\n    \n    for current_epoch in range(epochs):\n        total_training_loss = 0\n        correct_predictions = 0\n        total_samples = 0\n        \n        for batch_images, batch_labels in train_dataloader:\n            # Forward pass: get predictions\n            predicted_logits = model(batch_images)\n            training_loss = loss_function(predicted_logits, batch_labels)\n            \n            # Backward pass: update weights\n            optimizer.zero_grad()\n            accelerator.backward(training_loss)\n            optimizer.step()\n            \n            # Calculate training accuracy\n            predicted_classes = torch.argmax(predicted_logits, dim=1)\n            correct_predictions += (predicted_classes == batch_labels).sum().item()\n            total_samples += batch_labels.size(0)\n            total_training_loss += training_loss.item()\n        \n        # Calculate averages\n        average_training_loss = total_training_loss / len(train_dataloader)\n        training_accuracy = (correct_predictions / total_samples) * 100\n        \n        accelerator.print(f'Epoch {current_epoch+1:2d}: Training Loss = {average_training_loss:.4f}, Training Accuracy = {training_accuracy:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:36:26.825462Z","iopub.execute_input":"2025-09-04T05:36:26.826496Z","iopub.status.idle":"2025-09-04T05:36:26.833741Z","shell.execute_reply.started":"2025-09-04T05:36:26.826467Z","shell.execute_reply":"2025-09-04T05:36:26.832693Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def evaluate_classification_model(model, test_dataloader, accelerator):\n    model.eval()  # Set model to evaluation mode\n    \n    total_correct_predictions = 0\n    total_test_samples = 0\n    all_predictions = []\n    all_true_labels = []\n    \n    # Disable gradient computation for evaluation\n    with torch.no_grad():\n        for test_images, test_labels in test_dataloader:\n            # Get predictions\n            test_predictions_logits = model(test_images)\n            predicted_digit_classes = torch.argmax(test_predictions_logits, dim=1)\n            \n            # Collect predictions for detailed analysis\n            all_predictions.extend(predicted_digit_classes.cpu().numpy())\n            all_true_labels.extend(test_labels.cpu().numpy())\n            \n            # Calculate accuracy using torchmetrics\n            batch_accuracy = torchmetrics.functional.accuracy(\n                test_predictions_logits, test_labels, \n                task='multiclass', num_classes=10\n            )\n            \n            total_correct_predictions += batch_accuracy * test_labels.size(0)\n            total_test_samples += test_labels.size(0)\n    \n    # Calculate final test accuracy\n    final_test_accuracy = (total_correct_predictions / total_test_samples) * 100\n    accelerator.print(f'Final Test Accuracy: {final_test_accuracy:.2f}%')\n    \n    return all_predictions, all_true_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:36:39.666421Z","iopub.execute_input":"2025-09-04T05:36:39.666757Z","iopub.status.idle":"2025-09-04T05:36:39.675279Z","shell.execute_reply.started":"2025-09-04T05:36:39.666732Z","shell.execute_reply":"2025-09-04T05:36:39.674217Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def visualize_predictions(model, test_dataloader, accelerator, num_samples=16):\n    \"\"\"Visualize some predictions to understand model performance\"\"\"\n    model.eval()\n    \n    # Get one batch for visualization\n    test_images, test_labels = next(iter(test_dataloader))\n    \n    with torch.no_grad():\n        prediction_logits = model(test_images)\n        predicted_classes = torch.argmax(prediction_logits, dim=1)\n    \n    # Convert to CPU for plotting\n    images_cpu = test_images.cpu()\n    true_labels_cpu = test_labels.cpu()\n    predicted_labels_cpu = predicted_classes.cpu()\n    \n    # Create visualization\n    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n    for i, ax in enumerate(axes.flat):\n        if i < num_samples:\n            # Display image\n            ax.imshow(images_cpu[i].squeeze(), cmap='gray')\n            \n            # Add title with true and predicted labels\n            true_digit = true_labels_cpu[i].item()\n            predicted_digit = predicted_labels_cpu[i].item()\n            \n            # Color: green if correct, red if incorrect\n            title_color = 'green' if true_digit == predicted_digit else 'red'\n            ax.set_title(f'True: {true_digit}, Pred: {predicted_digit}', color=title_color)\n        \n        ax.axis('off')\n    \n    plt.suptitle('MNIST Classification Results (Green=Correct, Red=Incorrect)', fontsize=14)\n    plt.tight_layout()\n    plt.savefig('mnist_classification_results.png', dpi=150, bbox_inches='tight')\n    plt.close()\n    \n    accelerator.print(\"Saved visualization to 'mnist_classification_results.png'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:36:57.977477Z","iopub.execute_input":"2025-09-04T05:36:57.978406Z","iopub.status.idle":"2025-09-04T05:36:57.987372Z","shell.execute_reply.started":"2025-09-04T05:36:57.978373Z","shell.execute_reply":"2025-09-04T05:36:57.986365Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def main():\n    accelerator = Accelerator()\n    accelerator.print(\"Starting MNIST Digit Classification Training\")\n    \n    # Step 1: Load MNIST data\n    train_dataloader, test_dataloader = load_mnist_classification_data(batch_size=128)\n    accelerator.print(f\"Loaded MNIST: {len(train_dataloader)} train batches, {len(test_dataloader)} test batches\")\n    \n    # Step 2: Create classification model\n    classification_model = DigitClassificationNetwork(\n        input_size=784,      # 28x28 pixels\n        hidden_size_1=256,   # First hidden layer\n        hidden_size_2=128,   # Second hidden layer  \n        num_classes=10       # Digits 0-9\n    )\n    \n    # Step 3: Create optimizer\n    model_optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.001)\n    \n    # Step 4: Prepare everything with Accelerate\n    classification_model, model_optimizer, train_dataloader, test_dataloader = accelerator.prepare(\n        classification_model, model_optimizer, train_dataloader, test_dataloader\n    )\n    \n    total_parameters = sum(p.numel() for p in classification_model.parameters())\n    accelerator.print(f'Model has {total_parameters:,} trainable parameters')\n    \n    accelerator.print(\"\\nChoose training mode:\")\n    accelerator.print(\"1. Overfit single batch (testing mode)\")\n    accelerator.print(\"2. Full dataset training\")\n    \n    # OPTION 1: Test if network can learn by overfitting single batch\n    overfit_single_batch_classification(\n        classification_model, train_dataloader, model_optimizer, accelerator, iterations=200\n    )\n    \n    # Uncomment the lines below to run full training instead:\n    # train_classification_model(\n    #     classification_model, train_dataloader, model_optimizer, accelerator, epochs=15\n    # )\n    # \n    # accelerator.print(\"\\nEvaluating model on test data...\")\n    # all_predictions, all_true_labels = evaluate_classification_model(\n    #     classification_model, test_dataloader, accelerator\n    # )\n    # \n    # visualize_predictions(classification_model, test_dataloader, accelerator)\n    \n    accelerator.print(\"Training completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:37:12.385585Z","iopub.execute_input":"2025-09-04T05:37:12.385950Z","iopub.status.idle":"2025-09-04T05:37:12.395665Z","shell.execute_reply.started":"2025-09-04T05:37:12.385924Z","shell.execute_reply":"2025-09-04T05:37:12.393581Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T05:37:15.294572Z","iopub.execute_input":"2025-09-04T05:37:15.294969Z","iopub.status.idle":"2025-09-04T05:37:16.594477Z","shell.execute_reply.started":"2025-09-04T05:37:15.294942Z","shell.execute_reply":"2025-09-04T05:37:16.592973Z"}},"outputs":[{"name":"stdout","text":"Starting MNIST Digit Classification Training\nLoaded MNIST: 469 train batches, 79 test batches\nCreated classification network: 784 -> 256 -> 128 -> 10\nModel has 235,146 trainable parameters\n\nChoose training mode:\n1. Overfit single batch (testing mode)\n2. Full dataset training\n=== OVERFITTING TEST: Training classifier on single batch ===\nUsing single batch with 128 images\nDigits in this batch: [0 1 2 3 4 5 6 7 8 9]\nIteration  25: Loss = 0.4294, Accuracy = 94.5%\nIteration  50: Loss = 0.0249, Accuracy = 100.0%\nIteration  75: Loss = 0.0090, Accuracy = 100.0%\nIteration 100: Loss = 0.0057, Accuracy = 100.0%\nIteration 125: Loss = 0.0034, Accuracy = 100.0%\nIteration 150: Loss = 0.0027, Accuracy = 100.0%\nIteration 175: Loss = 0.0017, Accuracy = 100.0%\nIteration 200: Loss = 0.0015, Accuracy = 100.0%\nFinal overfitting accuracy: 100.0%\nIf accuracy reaches ~100%, the network can learn. If not, check architecture/learning rate.\nTraining completed!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}